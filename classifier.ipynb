{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing - Assignment 2\n",
    "#### November 2018\n",
    "\n",
    "###### Index Number: 14000334"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer (a)\n",
    "\n",
    "- Both negative and positive reviews read as two dataframes using pandas\n",
    "- Dataframes are concatenate the positive and negative review to one dataframe\n",
    "- Train and test dataframes are written into two CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Created\n",
      "\n",
      "Train Data Size: 25000\n",
      "Test Data size: 25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "import warnings\n",
    "\n",
    "# Removing future warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "negReviews = 12500\n",
    "posReviews = 12500\n",
    "\n",
    "# Reading the negative reviews train data\n",
    "negData = []\n",
    "negDataDir = 'aclImdb/train/neg/'\n",
    "for i in range(negReviews):\n",
    "    fileName = glob(negDataDir+str(i)+'_*.txt')[0]\n",
    "    f = open(fileName, \"r\", encoding=\"utf8\")\n",
    "    lines = f.readlines()\n",
    "    negData.append([lines[0],'0'])\n",
    "negDataFrame = pd.DataFrame(negData,columns=['review','sentiment'])\n",
    "\n",
    "# Reading the positive reviews train data\n",
    "posData = []\n",
    "posDataDir = 'aclImdb/train/pos/'\n",
    "for i in range(posReviews):\n",
    "    fileName = glob(posDataDir+str(i)+'_*.txt')[0]\n",
    "    f = open(fileName, \"r\", encoding=\"utf8\")\n",
    "    lines = f.readlines()\n",
    "    posData.append([lines[0],'1'])\n",
    "posDataFrame = pd.DataFrame(posData,columns=['review','sentiment'])\n",
    "\n",
    "trainDataFrame = pd.concat([posDataFrame,negDataFrame])\n",
    "\n",
    "# Writting the Train data into a CSV\n",
    "trainDataFrame.to_csv('train.csv', sep=',', encoding='utf-8', index=False)\n",
    "\n",
    "# Reading the negative reviews test data\n",
    "negData = []\n",
    "negDataDir = 'aclImdb/test/neg/'\n",
    "for i in range(negReviews):\n",
    "    fileName = glob(negDataDir+str(i)+'_*.txt')[0]\n",
    "    f = open(fileName, \"r\", encoding=\"utf8\")\n",
    "    lines = f.readlines()\n",
    "    negData.append([lines[0],'0'])\n",
    "negDataFrame = pd.DataFrame(negData,columns=['review','sentiment'])\n",
    "\n",
    "# Reading the positive reviews test data\n",
    "posData = []\n",
    "posDataDir = 'aclImdb/test/pos/'\n",
    "for i in range(posReviews):\n",
    "    fileName = glob(posDataDir+str(i)+'_*.txt')[0]\n",
    "    f = open(fileName, \"r\", encoding=\"utf8\")\n",
    "    lines = f.readlines()\n",
    "    posData.append([lines[0],'1'])\n",
    "posDataFrame = pd.DataFrame(posData,columns=['review','sentiment'])\n",
    "\n",
    "testDataFrame = pd.concat([posDataFrame,negDataFrame])\n",
    "\n",
    "# Writting the Test data into a CSV\n",
    "testDataFrame.to_csv('test.csv', sep=',', encoding='utf-8', index=False)\n",
    "print(\"Dataset Created\\n\")\n",
    "print(\"Train Data Size: %d\\nTest Data size: %d\\n\" % ((negReviews+posReviews),(negReviews+posReviews)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12500 negative reviews and 12500 positives reviews are used to create the train dataset <br>\n",
    "Same number of reviews are used to create the test dataset <br>\n",
    "Train data: <b>25000 reviews </b><br>\n",
    "Test data: <b>25000 reviews</b>\n",
    "\n",
    "#### ------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer (b)\n",
    "\n",
    "- Loaded the data from the CSV files to dataframes\n",
    "- Removed the accented charaters using 'unidecode'\n",
    "- Unescaping HTML escape sequences\n",
    "- Removed html tags using 'BeautifulSoup'\n",
    "- Expanded language contractions using custom contraction library\n",
    "- Removed Special characters\n",
    "- Removed words that present in less than five reviews\n",
    "- Cleaned reviews are written into CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens:\n",
      "\tTraining: 137388\n",
      "\tTesting: 134517\n",
      "\n",
      "Number of Tokens(removed less frequent words):\n",
      "\tTraining: 22011\n",
      "\tTesting: 21811\n"
     ]
    }
   ],
   "source": [
    "import unidecode\n",
    "import html\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "contractions = { \n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he will have\",\n",
    "    \"he's\": \"he has\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how does\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'd've\": \"I would have\",\n",
    "    \"I'll\": \"I shall\",\n",
    "    \"I'll've\": \"I shall have\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it has\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she has\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so is\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "# Expanding contractions\n",
    "contractions_re = re.compile('(%s)' % '|'.join(contractions.keys()))\n",
    "def expandContractions(s, contractions=contractions):\n",
    "    def replace(match):\n",
    "        return contractions[match.group(0)]\n",
    "    return contractions_re.sub(replace, s)\n",
    "\n",
    "# removing special characters (removed all characters except the alphabet)\n",
    "def removeSpecialChars(review):\n",
    "    return re.sub('[^A-Za-z ]', '', review)\n",
    "\n",
    "# cleaning the reviews\n",
    "def cleanReview(review):\n",
    "    # Normalizing accented characters\n",
    "    review = unidecode.unidecode(review)\n",
    "    # Unescaping HTML escape sequences\n",
    "    review = html.unescape(review)\n",
    "    # Removing extraneous html tags\n",
    "    review = BeautifulSoup(review).get_text()\n",
    "    # Expanding contractions\n",
    "    review = expandContractions(review)\n",
    "    # Removing special characters\n",
    "    review = removeSpecialChars(review)\n",
    "    # return the cleaned review\n",
    "    return review\n",
    "\n",
    "# return the words larger than the given frequency\n",
    "def getTokens(reviews, freq=0):\n",
    "    vec = CountVectorizer().fit((reviews).values.astype('U'))\n",
    "    bag_of_words = vec.transform(reviews)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    tokens = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    tokens =sorted(tokens, key = lambda x: x[1], reverse=True)\n",
    "    tokens = [token for (token, frequency) in tokens if frequency > freq]\n",
    "    return tokens\n",
    "\n",
    "# return review by removing words that present in less than five reviews\n",
    "def rmvLessFreqTokens(review, freqList):\n",
    "    result = ' '.join(list(filter(lambda x: x in freqList, review.split(' '))))\n",
    "    if result == '':\n",
    "        result = 'review'\n",
    "    return result\n",
    "    \n",
    "\n",
    "# Loading csv files data into dataframes\n",
    "trainDataFrame = pd.read_csv('train.csv', sep=',')\n",
    "testDataFrame = pd.read_csv('test.csv', sep=',')\n",
    "\n",
    "# cleaning the train and test reviews\n",
    "try:\n",
    "    trainDataFrame['review'] = trainDataFrame['review'].apply(lambda review:cleanReview(review))\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    testDataFrame['review'] = testDataFrame['review'].apply(lambda review:cleanReview(review))\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "# Counting the number of training and testing data set tokens   \n",
    "trainTokens = getTokens(trainDataFrame['review'],0)\n",
    "testTokens = getTokens(testDataFrame['review'],0)\n",
    "\n",
    "print('Number of Tokens:')\n",
    "print('\\tTraining:', len(trainTokens))\n",
    "print('\\tTesting:', len(testTokens))\n",
    "\n",
    "# getting tokens that have frequency more than 5 in training data set\n",
    "trainTokens = getTokens(trainDataFrame['review'],5)\n",
    "# getting tokens that have frequency more than 5 in testing data set\n",
    "testTokens = getTokens(testDataFrame['review'],5)\n",
    "\n",
    "# Removing the less frequent words from the train and test data sets\n",
    "try:\n",
    "    trainDataFrame['review'] = trainDataFrame['review'].apply(lambda review:rmvLessFreqTokens(review,trainTokens))\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    testDataFrame['review'] = testDataFrame['review'].apply(lambda review:rmvLessFreqTokens(review,testTokens))\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "# Counting the number of training and testing data set tokens   \n",
    "trainTokens = getTokens(trainDataFrame['review'],0)\n",
    "testTokens = getTokens(testDataFrame['review'],0)\n",
    "\n",
    "print('\\nNumber of Tokens(removed less frequent words):')\n",
    "print('\\tTraining:', len(trainTokens))\n",
    "print('\\tTesting:', len(testTokens))\n",
    "\n",
    "\n",
    "# Writting the processed dataframes to CSV files\n",
    "trainDataFrame.to_csv('trainDataset1.csv', sep=',', encoding='utf-8', index=False)\n",
    "testDataFrame.to_csv('testDataset1.csv', sep=',', encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of tokens reduced after processing the dataset\n",
    "\n",
    "#### ------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer (c)\n",
    "\n",
    "- Loaded data from CSV files\n",
    "- Removed stopwords from the reviews\n",
    "- Created second dataset by removing stop words\n",
    "- Lemmatizating the data\n",
    "- Created third dataset\n",
    "- Create separate SVM for three datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Tokens(removed less frequent words):\n",
      "\tTraining: 22011\n",
      "\tTesting: 21811\n",
      "\n",
      "Number of Tokens(stop words removed):\n",
      "\tTraining: 21880\n",
      "\tTesting: 21679\n",
      "\n",
      "Number of Tokens(lemmatized):\n",
      "\tTraining: 18773\n",
      "\tTesting: 18619\n",
      "\n",
      "*** Results from the models ***\n",
      "\n",
      "Dataset 1:\n",
      "\tCross Validation Scores: [0.8276 0.8342 0.8264 0.834  0.8342]\n",
      "\tModel Accuracy: 0.83 (+/- 0.01)\n",
      "\tTest Accuracy: 0.84\n",
      "\n",
      "Dataset 2:\n",
      "\tCross Validation Scores: [0.831  0.8276 0.8248 0.8282 0.8304]\n",
      "\tModel Accuracy: 0.83 (+/- 0.00)\n",
      "\tTest Accuracy: 0.84\n",
      "\n",
      "Dataset 3:\n",
      "\tCross Validation Scores: [0.8286 0.8358 0.8164 0.8212 0.832 ]\n",
      "\tModel Accuracy: 0.83 (+/- 0.01)\n",
      "\tTest Accuracy: 0.83\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Removing convergence warnings\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "\n",
    "\n",
    "# getting the stop words\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "# Reduce the noice of the given review\n",
    "def reduceNoice(review):\n",
    "    # tokenize the input\n",
    "    tokens = word_tokenize(review)\n",
    "    # removing the stop words\n",
    "    filteredTokens = [word for word in tokens if not word in stopWords] \n",
    "    # converting the token array into a string\n",
    "    filteredTokens = ' '.join(filteredTokens)\n",
    "    return filteredTokens\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(review):\n",
    "    # tokenize the input\n",
    "    tokens = word_tokenize(review)\n",
    "    # lemmatizing\n",
    "    lemmatizedTokens = [lemmatizer.lemmatize(word) for word in tokens] \n",
    "    lemmatizedTokens = ' '.join(lemmatizedTokens)\n",
    "    return lemmatizedTokens\n",
    "\n",
    "# return the words larger than the given frequency\n",
    "def getTokens(reviews, freq=0):\n",
    "    vec = CountVectorizer().fit((reviews).values.astype('U'))\n",
    "    bag_of_words = vec.transform(reviews)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    tokens = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    tokens =sorted(tokens, key = lambda x: x[1], reverse=True)\n",
    "    tokens = [token for (token, frequency) in tokens if frequency > freq]\n",
    "    return tokens\n",
    "\n",
    "# Loading csv files data into dataframes\n",
    "trainDataFrame = pd.read_csv('trainDataset1.csv', sep=',')\n",
    "testDataFrame = pd.read_csv('testDataset1.csv', sep=',')\n",
    "    \n",
    "# Adding a new dataframe  \n",
    "trainDataFrame2 = pd.DataFrame([],columns=['review'])\n",
    "trainDataFrame2['review'] = trainDataFrame['review']\n",
    "\n",
    "testDataFrame2 = pd.DataFrame([],columns=['review'])\n",
    "testDataFrame2['review'] = testDataFrame['review']\n",
    "\n",
    "# reducing the noice train and test reviews\n",
    "try:\n",
    "    trainDataFrame2['review'] = trainDataFrame2['review'].apply(lambda review:reduceNoice(review))\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    testDataFrame2['review'] = testDataFrame2['review'].apply(lambda review:reduceNoice(review))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Writting the processed dataframes to CSV files\n",
    "trainDataFrame2.to_csv('trainDataset2.csv', sep=',', encoding='utf-8', index=False)\n",
    "testDataFrame2.to_csv('testDataset2.csv', sep=',', encoding='utf-8', index=False)\n",
    "\n",
    "\n",
    "# Adding a new dataframe  \n",
    "trainDataFrame3 = pd.DataFrame([],columns=['review'])\n",
    "trainDataFrame3['review'] = trainDataFrame2['review']\n",
    "\n",
    "testDataFrame3 = pd.DataFrame([],columns=['review'])\n",
    "testDataFrame3['review'] = testDataFrame2['review']\n",
    "\n",
    "# stemming and lemmatizing the noice train and test reviews\n",
    "try:\n",
    "    trainDataFrame3['review'] = trainDataFrame3['review'].apply(lambda review:lemmatize(review))\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    testDataFrame3['review'] = testDataFrame3['review'].apply(lambda review:lemmatize(review))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Writting the processed dataframes to CSV files\n",
    "trainDataFrame3.to_csv('trainDataset3.csv', sep=',', encoding='utf-8', index=False)\n",
    "testDataFrame3.to_csv('testDataset3.csv', sep=',', encoding='utf-8', index=False)\n",
    "\n",
    "# Counting the number of training and testing data set tokens   \n",
    "trainTokens = getTokens(trainDataFrame['review'],0)\n",
    "testTokens = getTokens(testDataFrame['review'],0)\n",
    "\n",
    "print('\\nNumber of Tokens(removed less frequent words):')\n",
    "print('\\tTraining:', len(trainTokens))\n",
    "print('\\tTesting:', len(testTokens))\n",
    "\n",
    "trainTokens = getTokens(trainDataFrame2['review'],0)\n",
    "testTokens = getTokens(testDataFrame2['review'],0)\n",
    "\n",
    "print('\\nNumber of Tokens(stop words removed):')\n",
    "print('\\tTraining:', len(trainTokens))\n",
    "print('\\tTesting:', len(testTokens))\n",
    "\n",
    "trainTokens = getTokens(trainDataFrame3['review'],0)\n",
    "testTokens = getTokens(testDataFrame3['review'],0)\n",
    "\n",
    "print('\\nNumber of Tokens(lemmatized):')\n",
    "print('\\tTraining:', len(trainTokens))\n",
    "print('\\tTesting:', len(testTokens))\n",
    "\n",
    "trainDataFrames = [trainDataFrame,trainDataFrame2,trainDataFrame3]\n",
    "testDataFrames = [testDataFrame,testDataFrame2,testDataFrame3]\n",
    "\n",
    "# Generating SVMs and evaluating the models\n",
    "print(\"\\n*** Results from the models ***\\n\")\n",
    "for datasetIndx in range(len(trainDataFrames)):\n",
    "    print(\"Dataset %d:\"%(datasetIndx+1))\n",
    "    vectorizer = CountVectorizer() \n",
    "    vectorizer.fit(trainDataFrames[datasetIndx]['review'].values.astype('U'))\n",
    "    # vectorizing the training set\n",
    "    trainData = vectorizer.transform(trainDataFrames[datasetIndx]['review'].values.astype('U'))\n",
    "    # vectorizing the testing set\n",
    "    testData = vectorizer.transform(testDataFrames[datasetIndx]['review'])\n",
    "    # training the classifier\n",
    "    classifier = svm.LinearSVC()\n",
    "    classifier.fit(trainData, trainDataFrame['sentiment'])\n",
    "    # cross validation\n",
    "    scores = cross_val_score(classifier, trainData, trainDataFrame['sentiment'], cv=5, n_jobs=-1)\n",
    "    print(\"\\tCross Validation Scores:\",scores)\n",
    "    print(\"\\tModel Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    # testing \n",
    "    predictionScore = classifier.score(testData,testDataFrame['sentiment'])\n",
    "    print(\"\\tTest Accuracy: %0.2f\\n\" % predictionScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Model 1:</b> Words that present in less than five reviews removed (Dataset 1)  <br>\n",
    "<b>Model 2:</b> Stop words removed (Dataset 2) <br>\n",
    "<b>Model 3:</b> Lemmatized (Dataset 3) <br><br>\n",
    "\n",
    "No significance difference between cross validation scores and the test accuracies of the 3 models <br>\n",
    "SVM models 1 and 2 have same accuracies <br>\n",
    "Accuracy: <b>84%</b> <br>\n",
    "Model 3 has low accuracy compared to other two models <br>\n",
    "Accuracy: <b>83%</b>   \n",
    "</b>\n",
    "#### ------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer (d)\n",
    "- SVM model using tf-idf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Using TFIDF values ***\n",
      "\n",
      "Cross Validation Scores: [0.8584 0.8634 0.8484 0.8564 0.8616]\n",
      "Model Accuracy: 0.86 (+/- 0.01)\n",
      "Test Accuracy: 0.87\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print(\"\\n*** Using TFIDF values ***\\n\")\n",
    "datasetIndx = 2\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(trainDataFrames[datasetIndx]['review'].values.astype('U'))\n",
    "# vectorizing the training set\n",
    "trainData = vectorizer.transform(trainDataFrames[datasetIndx]['review'].values.astype('U'))\n",
    "# vectorizing the testing set\n",
    "testData = vectorizer.transform(testDataFrames[datasetIndx]['review'])\n",
    "# training the classifier\n",
    "classifier = svm.LinearSVC()\n",
    "classifier.fit(trainData, trainDataFrame['sentiment'])\n",
    "# cross validation\n",
    "scores = cross_val_score(classifier, trainData, trainDataFrame['sentiment'], cv=5, n_jobs=-1)\n",
    "print(\"Cross Validation Scores:\",scores)\n",
    "print(\"Model Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "# testing \n",
    "predictionScore = classifier.score(testData,testDataFrame['sentiment'])\n",
    "print(\"Test Accuracy: %0.2f\\n\" % predictionScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used term frequency–inverse document frequency values <br>\n",
    "Cross validation scores are in the range <b> 0.85 - 0.86 </b> <br>\n",
    "No significance difference between cross validation scores and the test accuracies of the 3 models <br>\n",
    "Model with highest accuray when compared to all the models <br>\n",
    "Accuracy: <b>87%</b>   \n",
    "\n",
    "#### ------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer (e)\n",
    "\n",
    "- SVM Using tf-idf values and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Using TFIDF values and bigrams ***\n",
      "\n",
      "Cross Validation Scores: [0.825  0.825  0.8226 0.826  0.8296]\n",
      "Model Accuracy: 0.83 (+/- 0.00)\n",
      "Test Accuracy: 0.84\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "print(\"\\n*** Using TFIDF values and bigrams ***\\n\")\n",
    "datasetIndx = 2\n",
    "vectorizer = TfidfVectorizer(min_df=1, ngram_range=(2, 2))\n",
    "vectorizer.fit(trainDataFrames[datasetIndx]['review'].values.astype('U'))\n",
    "# vectorizing the training set\n",
    "trainData = vectorizer.transform(trainDataFrames[datasetIndx]['review'].values.astype('U'))\n",
    "# vectorizing the testing set\n",
    "testData = vectorizer.transform(testDataFrames[datasetIndx]['review'])\n",
    "# training the classifier\n",
    "classifier = svm.LinearSVC()\n",
    "classifier.fit(trainData, trainDataFrame['sentiment'])\n",
    "# cross validation\n",
    "scores = cross_val_score(classifier, trainData, trainDataFrame['sentiment'], cv=5, n_jobs=-1)\n",
    "print(\"Cross Validation Scores:\",scores)\n",
    "print(\"Model Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "# testing \n",
    "predictionScore = classifier.score(testData,testDataFrame['sentiment'])\n",
    "print(\"Test Accuracy: %0.2f\\n\" % predictionScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used term frequency–inverse document frequency (tf-idf) values with bigrams <br>\n",
    "No significance difference between cross validation score and the test accuracy <br>\n",
    "Accuracy is low compared to SVM model with tf-idf with unigrams  <br>\n",
    "Accuracy is same as the SVM with bag-of-words feature model  <br>\n",
    "Accuracy: <b>84%</b> <br><br>\n",
    "\n",
    "- <b> Model Accuracies </b>\n",
    "\n",
    "<table align=\"left\">\n",
    "  <tr>\n",
    "    <th>SVM Model</th>\n",
    "    <th>Features</th>\n",
    "    <th>Cross Validation</th>\n",
    "    <th>Test Accuracy</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>Bag Of Words (cleaned)</td>\n",
    "    <td>.83</td>\n",
    "    <td>.84</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>Bag Of Words (stop words removed)</td>\n",
    "    <td>.83</td>\n",
    "    <td>.84</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td>Bag Of Words (lemmatized)</td>\n",
    "    <td>.83</td>\n",
    "    <td>.83</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>4</b></td>\n",
    "    <td><b>TF-IDF with unigrams</b></td>\n",
    "    <td><b>.86</b></td>\n",
    "    <td><b>.87</b></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>5</td>\n",
    "    <td>TF-IDF with bigrams</td>\n",
    "    <td>.83</td>\n",
    "    <td>.84</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "<br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "#### ------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer (f)\n",
    "\n",
    "- Logistic regression for the third dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Using Logistic regression ***\n",
      "\n",
      "Cross Validation Scores: [0.848  0.8508 0.8398 0.85   0.853 ]\n",
      "Model Accuracy: 0.85 (+/- 0.01)\n",
      "Test Accuracy: 0.86\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "print(\"\\n*** Using Logistic regression ***\\n\")\n",
    "\n",
    "datasetIndx = 2\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "vectorizer.fit(trainDataFrames[datasetIndx]['review'].values.astype('U'))\n",
    "# vectorizing the training set\n",
    "trainData = vectorizer.transform(trainDataFrames[datasetIndx]['review'].values.astype('U'))\n",
    "# vectorizing the testing set\n",
    "testData = vectorizer.transform(testDataFrames[datasetIndx]['review'])\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "# Train the model using default parameters\n",
    "classifier.fit(trainData, trainDataFrame['sentiment'])\n",
    "# cross validation\n",
    "scores = cross_val_score(classifier, trainData, trainDataFrame['sentiment'], cv=5, n_jobs=-1)\n",
    "print(\"Cross Validation Scores:\",scores)\n",
    "print(\"Model Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "# testing \n",
    "predictionScore = classifier.score(testData,testDataFrame['sentiment'])\n",
    "print(\"Test Accuracy: %0.2f\\n\" % predictionScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used Logistic Regression model with unigrams <br>\n",
    "No significance difference between cross validation score and the test accuracy <br>\n",
    "Second highest accuracy, only less than the SVM model with tf-idf with unigrams <br>\n",
    "Accuracy: <b>86%</b>\n",
    "\n",
    "#### ------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
